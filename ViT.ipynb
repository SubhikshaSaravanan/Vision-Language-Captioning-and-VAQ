{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import os\n",
        "import pandas as pd\n",
        "import glob\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
        "import torch\n",
        "from transformers import ViTForImageClassification\n",
        "from transformers import ViTModel\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "import concurrent.futures\n",
        "import csv\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import pipeline, MarianMTModel, MarianTokenizer\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer"
      ],
      "metadata": {
        "id": "7XEk8xnAk_qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = kagglehub.dataset_download(\"hsankesara/flickr-image-dataset\")\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tU-QWo8bjY-",
        "outputId": "40687025-d9ed-4ba1-8b19-0c4ab78495d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/flickr-image-dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main_dir = '/kaggle/input/flickr-image-dataset'"
      ],
      "metadata": {
        "id": "X33vvl-6dN31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flickr30k_images_folder = os.path.join(main_dir, 'flickr30k_images')\n",
        "image_files = glob.glob(f'{flickr30k_images_folder}/*')\n",
        "print(image_files[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUHKrLVWgOuf",
        "outputId": "5aa6523a-c8f5-4eeb-a121-5c96637f3b73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images', '/kaggle/input/flickr-image-dataset/flickr30k_images/results.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path = os.path.join(main_dir, 'flickr30k_images', 'results.csv')\n",
        "if not os.path.exists(csv_path):\n",
        "    print(f\"File not found: {csv_path}\")\n",
        "else:\n",
        "    df = pd.read_csv(csv_path, delimiter='|')\n",
        "    print(df.head())"
      ],
      "metadata": {
        "id": "RKzg_JUfYDh4",
        "outputId": "aeccd338-d977-4c5a-f96d-1c9440d0925f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       image_name  comment_number  \\\n",
            "0  1000092795.jpg               0   \n",
            "1  1000092795.jpg               1   \n",
            "2  1000092795.jpg               2   \n",
            "3  1000092795.jpg               3   \n",
            "4  1000092795.jpg               4   \n",
            "\n",
            "                                             comment  \n",
            "0   Two young guys with shaggy hair look at their...  \n",
            "1   Two young , White males are outside near many...  \n",
            "2   Two men in green shirts are standing in a yard .  \n",
            "3       A man in a blue shirt standing in a garden .  \n",
            "4            Two friends enjoy time spent together .  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path = os.path.join(main_dir, 'flickr30k_images', 'results.csv')\n",
        "df = pd.read_csv(csv_path, delimiter='|')\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOVwRkhUgkbr",
        "outputId": "70a7a6e1-cf5c-457b-9e89-2130ecc9fba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       image_name  comment_number  \\\n",
            "0  1000092795.jpg               0   \n",
            "1  1000092795.jpg               1   \n",
            "2  1000092795.jpg               2   \n",
            "3  1000092795.jpg               3   \n",
            "4  1000092795.jpg               4   \n",
            "\n",
            "                                             comment  \n",
            "0   Two young guys with shaggy hair look at their...  \n",
            "1   Two young , White males are outside near many...  \n",
            "2   Two men in green shirts are standing in a yard .  \n",
            "3       A man in a blue shirt standing in a garden .  \n",
            "4            Two friends enjoy time spent together .  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_folder = \"/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images\"\n",
        "image_files = os.listdir(image_folder)\n",
        "print(image_files[:10])"
      ],
      "metadata": {
        "id": "Z8IIizMDhJOa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "348c2c8d-00ad-440a-c32f-d773c1bc102a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['2715746315.jpg', '3463034205.jpg', '268704620.jpg', '2673564214.jpg', '7535037918.jpg', '4912369161.jpg', '4828071602.jpg', '6802728196.jpg', '3346289227.jpg', '3217056901.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_folder = \"/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images\"\n",
        "\n",
        "image_files = os.listdir(image_folder)[:5]\n",
        "fig, axes = plt.subplots(1, len(image_files), figsize=(15, 5))\n",
        "\n",
        "for i, img_file in enumerate(image_files):\n",
        "    img_path = os.path.join(image_folder, img_file)\n",
        "    img = Image.open(img_path)\n",
        "    axes[i].imshow(img)\n",
        "    axes[i].axis(\"off\")\n",
        "    axes[i].set_title(img_file)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KATOyDoOiDB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = 224\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    return transform(img)\n",
        "\n",
        "example_img_path = '/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/1000092795.jpg'\n",
        "image_tensor = preprocess_image(example_img_path)\n",
        "print(image_tensor.shape)"
      ],
      "metadata": {
        "id": "fxzhNjF1iWUy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3da8dc55-f8d1-4f73-aa94-a555e5e42bc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "tokenizer.pad_token = '[PAD]'\n",
        "caption = \"Two young guys with shaggy hair look at their hands while hanging out in the yard.\"\n",
        "tokenized_caption = tokenizer(caption, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "print(tokenized_caption['input_ids'])"
      ],
      "metadata": {
        "id": "_ShLYzr9jCRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5516528-1425-4a76-88e9-1b70dba9a352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  101,  2048,  2402,  4364,  2007, 25741,  2606,  2298,  2012,  2037,\n",
            "          2398,  2096,  5689,  2041,  1999,  1996,  4220,  1012,   102]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/vit-base-patch16-224-in21k\"\n",
        "model = ViTForImageClassification.from_pretrained(model_name)\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\n",
        "model.eval()\n",
        "image_tensor = image_tensor.unsqueeze(0)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(image_tensor)\n",
        "\n",
        "logits = outputs.logits\n",
        "probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
        "predicted_class_idx = torch.argmax(probabilities, dim=-1).item()\n",
        "predicted_class_prob = probabilities[0, predicted_class_idx].item()\n",
        "\n",
        "print(f\"Predicted class index: {predicted_class_idx}\")\n",
        "print(f\"Prediction probability: {predicted_class_prob}\")\n"
      ],
      "metadata": {
        "id": "yegryPiijoEj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9259d280-9859-4e0f-9d88-fa91904b340b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class index: 1\n",
            "Prediction probability: 0.5312654376029968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/vit-base-patch16-224-in21k\"\n",
        "model = ViTForImageClassification.from_pretrained(model_name)\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\n",
        "\n",
        "def extract_features(image_path):\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = feature_extractor(img, return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    features = outputs.logits\n",
        "    return features\n",
        "\n",
        "example_img_path = '/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/1000092795.jpg'\n",
        "image_features = extract_features(example_img_path)\n",
        "print(image_features.shape)"
      ],
      "metadata": {
        "id": "YVXa0_sGlTpM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de1b56d8-8f5b-4080-bebd-51a403057b02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/vit-base-patch16-224-in21k\"\n",
        "model = ViTModel.from_pretrained(model_name)\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\n",
        "\n",
        "def extract_features(image_path):\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = feature_extractor(img, return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    features = outputs.last_hidden_state\n",
        "    return features\n",
        "\n",
        "example_img_path = '/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/1000092795.jpg'\n",
        "image_features = extract_features(example_img_path)\n",
        "print(image_features.shape)"
      ],
      "metadata": {
        "id": "r-C41A6DmWTl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8125bc93-840a-4b83-bfa7-8ef0f04a8665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 197, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_features_cls = image_features[:, 0, :]\n",
        "print(image_features_cls.shape)"
      ],
      "metadata": {
        "id": "gtKgvU63nA8r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db6417f4-7cda-4f0c-94de-29b7a0fe1ade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "def generate_caption(image_features_cls):\n",
        "    raw_image = Image.open(example_img_path).convert(\"RGB\")\n",
        "    inputs = processor(raw_image, return_tensors=\"pt\")\n",
        "\n",
        "    out = model.generate(**inputs)\n",
        "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "    return caption\n",
        "\n",
        "caption = generate_caption(image_features_cls)\n",
        "print(\"Generated Caption:\", caption)"
      ],
      "metadata": {
        "id": "fEtdc1hHoQsK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4493d6b-b56d-4bea-b322-015809844d24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Caption: a man standing in the grass\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "def generate_caption(image_path):\n",
        "    raw_image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = processor(raw_image, return_tensors=\"pt\")\n",
        "    out = model.generate(**inputs)\n",
        "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "    return (image_path, caption)\n",
        "\n",
        "image_folder = \"/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images\"\n",
        "image_files = os.listdir(image_folder)\n",
        "num_images_to_process = 50\n",
        "image_caption_pairs = []\n",
        "\n",
        "def generate_caption_for_image(img_file, image_folder):\n",
        "    img_path = os.path.join(image_folder, img_file)\n",
        "    return generate_caption(img_path)\n",
        "\n",
        "def process_images_in_parallel(image_files, image_folder, num_images_to_process):\n",
        "    image_files_to_process = image_files[:num_images_to_process]\n",
        "\n",
        "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
        "        results = list(executor.map(generate_caption_for_image, image_files_to_process, [image_folder] * len(image_files_to_process)))\n",
        "\n",
        "    return results\n",
        "\n",
        "image_caption_pairs = process_images_in_parallel(image_files, image_folder, num_images_to_process)\n",
        "\n",
        "for img_path, caption in image_caption_pairs[:5]:\n",
        "    print(f\"Image: {img_path}\\nCaption: {caption}\\n\")"
      ],
      "metadata": {
        "id": "f40wI-OQ-jsy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7de5c864-f2ba-4ede-b4e8-e2c465f4e58c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: /kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/2715746315.jpg\n",
            "Caption: man wearing a hat\n",
            "\n",
            "Image: /kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/3463034205.jpg\n",
            "Caption: a skate park\n",
            "\n",
            "Image: /kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/268704620.jpg\n",
            "Caption: two dogs running in the snow\n",
            "\n",
            "Image: /kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/2673564214.jpg\n",
            "Caption: a blue and green tent\n",
            "\n",
            "Image: /kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/7535037918.jpg\n",
            "Caption: a man with a beard\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_file = 'image_captions.csv'\n",
        "with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Image Path', 'Caption'])\n",
        "    for img_path, caption in image_caption_pairs:\n",
        "        writer.writerow([img_path, caption])"
      ],
      "metadata": {
        "id": "MTWcsgj-D0Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "class ImageCaptionDataset(Dataset):\n",
        "    def __init__(self, image_caption_pairs, transform=None, tokenizer=None):\n",
        "        self.image_caption_pairs = image_caption_pairs\n",
        "        self.transform = transform\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_caption_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, caption = self.image_caption_pairs[idx]\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        caption_tokenized = self.tokenizer(caption, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "        return img, caption_tokenized.input_ids.squeeze(0)\n",
        "\n",
        "dataset = ImageCaptionDataset(image_caption_pairs, transform=transform, tokenizer=tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "for images, captions in dataloader:\n",
        "    print(f\"Images batch shape: {images.shape}\")\n",
        "    print(f\"Captions batch shape: {captions.shape}\")\n",
        "    break\n"
      ],
      "metadata": {
        "id": "fsowSScO8kqv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c944e8f-3acc-4069-c347-5c79b3adb9ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images batch shape: torch.Size([16, 3, 224, 224])\n",
            "Captions batch shape: torch.Size([16, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfile = pd.read_csv('/content/image_captions.csv')\n",
        "dfile.head(5)"
      ],
      "metadata": {
        "id": "AvXk_rbvJyPu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "48bde721-5831-41c5-f314-7c0a43ae8c37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          Image Path  \\\n",
              "0  /kaggle/input/flickr-image-dataset/flickr30k_i...   \n",
              "1  /kaggle/input/flickr-image-dataset/flickr30k_i...   \n",
              "2  /kaggle/input/flickr-image-dataset/flickr30k_i...   \n",
              "3  /kaggle/input/flickr-image-dataset/flickr30k_i...   \n",
              "4  /kaggle/input/flickr-image-dataset/flickr30k_i...   \n",
              "\n",
              "                        Caption  \n",
              "0             man wearing a hat  \n",
              "1                  a skate park  \n",
              "2  two dogs running in the snow  \n",
              "3         a blue and green tent  \n",
              "4            a man with a beard  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ac3cbfeb-6578-4a7f-b1d4-47ad9f2560d7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image Path</th>\n",
              "      <th>Caption</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/kaggle/input/flickr-image-dataset/flickr30k_i...</td>\n",
              "      <td>man wearing a hat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/kaggle/input/flickr-image-dataset/flickr30k_i...</td>\n",
              "      <td>a skate park</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/kaggle/input/flickr-image-dataset/flickr30k_i...</td>\n",
              "      <td>two dogs running in the snow</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/kaggle/input/flickr-image-dataset/flickr30k_i...</td>\n",
              "      <td>a blue and green tent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/kaggle/input/flickr-image-dataset/flickr30k_i...</td>\n",
              "      <td>a man with a beard</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ac3cbfeb-6578-4a7f-b1d4-47ad9f2560d7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ac3cbfeb-6578-4a7f-b1d4-47ad9f2560d7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ac3cbfeb-6578-4a7f-b1d4-47ad9f2560d7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-8bc0ec48-d4ea-4f53-a4ec-89b25e5f057a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8bc0ec48-d4ea-4f53-a4ec-89b25e5f057a')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-8bc0ec48-d4ea-4f53-a4ec-89b25e5f057a button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dfile",
              "summary": "{\n  \"name\": \"dfile\",\n  \"rows\": 50,\n  \"fields\": [\n    {\n      \"column\": \"Image Path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/2700788458.jpg\",\n          \"/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/3135826945.jpg\",\n          \"/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/3173976185.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Caption\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 49,\n        \"samples\": [\n          \"a man and a woman playing frc\",\n          \"a woman holding an umbrella\",\n          \"a man holding a silver ball\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BktZC0W4a1P_",
        "outputId": "612290ff-874f-412b-ce63-dfef34e755dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.27.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.9.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.9.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.7)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.9.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.9.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "caption_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "caption_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "languages = {\n",
        "    \"French\": \"Helsinki-NLP/opus-mt-en-fr\",\n",
        "    \"Spanish\": \"Helsinki-NLP/opus-mt-en-es\",\n",
        "    \"Hindi\": \"Helsinki-NLP/opus-mt-en-hi\",\n",
        "    \"Mandarin\": \"Helsinki-NLP/opus-mt-en-zh\",\n",
        "}\n",
        "\n",
        "story_generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
        "\n",
        "\n",
        "def generate_caption(image):\n",
        "    inputs = caption_processor(image, return_tensors=\"pt\")\n",
        "    out = caption_model.generate(**inputs)\n",
        "    caption = caption_processor.decode(out[0], skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "\n",
        "def translate_caption(caption, target_lang):\n",
        "    model_name = languages.get(target_lang)\n",
        "    if model_name is None:\n",
        "        return caption\n",
        "\n",
        "    model = MarianMTModel.from_pretrained(model_name)\n",
        "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    try:\n",
        "        translated = model.generate(**tokenizer(caption, return_tensors=\"pt\"))\n",
        "        translated_caption = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
        "        return translated_caption\n",
        "    except Exception as e:\n",
        "        return f\"Translation error: {str(e)}\"\n",
        "\n",
        "\n",
        "def multilingual_caption(image, language):\n",
        "    english_caption = generate_caption(image)\n",
        "\n",
        "    if language != 'English':\n",
        "        translated_caption = translate_caption(english_caption, language)\n",
        "    else:\n",
        "        translated_caption = english_caption\n",
        "\n",
        "    return f\"English: {english_caption}\\n{language.capitalize()}: {translated_caption}\"\n",
        "\n",
        "\n",
        "def generate_story(image):\n",
        "    caption = generate_caption(image)\n",
        "    story_prompt = f\"Based on the following scene: '{caption}', tell me a detailed, creative, and coherent story with a beginning, middle, and end.\"\n",
        "    story = story_generator(story_prompt, max_length=200, do_sample=True)[0]['generated_text']\n",
        "    return story\n",
        "\n",
        "\n",
        "def answer_question(image, question):\n",
        "    caption = generate_caption(image)\n",
        "    qa_input = {\"question\": question, \"context\": caption}\n",
        "    answer = qa_pipeline(qa_input)\n",
        "    return answer['answer']\n",
        "\n",
        "\n",
        "def process_image(img, option, language=\"English\", question=\"\"):\n",
        "    if option == \"Storytelling\":\n",
        "        return generate_story(img)\n",
        "    elif option == \"Multilingual Caption\":\n",
        "        return multilingual_caption(img, language)\n",
        "    elif option == \"Q&A\":\n",
        "        if not question:\n",
        "            return \"Please enter a question.\"\n",
        "        return answer_question(img, question)\n",
        "    else:\n",
        "        return \"Invalid option.\"\n"
      ],
      "metadata": {
        "id": "BdlzXHQfCmdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beca1744-69ea-4140-d8bf-15dea897bada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as iface:\n",
        "    gr.Markdown(\"## AI-Powered Image Processing\")\n",
        "\n",
        "    with gr.Row():\n",
        "        image_input = gr.Image(type=\"pil\", label=\"Upload an Image\")\n",
        "\n",
        "    with gr.Row():\n",
        "        option_dropdown = gr.Dropdown(\n",
        "            choices=[\"Storytelling\", \"Multilingual Caption\", \"Q&A\"],\n",
        "            label=\"Choose an Option\"\n",
        "        )\n",
        "\n",
        "    with gr.Row():\n",
        "        language_input = gr.Dropdown(\n",
        "            choices=[\"English\", \"French\", \"Spanish\", \"Hindi\", \"Mandarin\"],\n",
        "            label=\"Choose a Language for Translation\",\n",
        "            visible=False\n",
        "        )\n",
        "\n",
        "        question_input = gr.Textbox(label=\"Enter a question (For Q&A only)\", visible=False)\n",
        "\n",
        "    output_display = gr.Textbox(label=\"Output\", interactive=False)\n",
        "\n",
        "    def show_language_box(option):\n",
        "        return gr.update(visible=(option == \"Multilingual Caption\"))\n",
        "\n",
        "    def show_question_box(option):\n",
        "        return gr.update(visible=(option == \"Q&A\"))\n",
        "\n",
        "    option_dropdown.change(show_language_box, inputs=[option_dropdown], outputs=[language_input])\n",
        "    option_dropdown.change(show_question_box, inputs=[option_dropdown], outputs=[question_input])\n",
        "\n",
        "    process_button = gr.Button(\"Process\")\n",
        "    process_button.click(\n",
        "        process_image,\n",
        "        inputs=[image_input, option_dropdown, language_input, question_input],\n",
        "        outputs=output_display\n",
        "    )\n",
        "\n",
        "iface.launch(share=True)"
      ],
      "metadata": {
        "id": "oh3--1TGU2Kb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "outputId": "ad1092c2-e3b2-4a66-f3fb-97e52d0164b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://be4f74754d43c5df31.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://be4f74754d43c5df31.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reference_captions = [\n",
        "    \"A man and a boy are standing before the sky\",\n",
        "    \"A girl standing in front of a poster banner\"\n",
        "]\n",
        "\n",
        "generated_captions = [\n",
        "    \"A young girl looking at the sky staring at a star\",\n",
        "    \"A young girl standing before a banner.\"\n",
        "]\n",
        "\n",
        "ref_tokens = [set(ref.lower().split()) for ref in reference_captions]\n",
        "gen_tokens = [set(gen.lower().split()) for gen in generated_captions]\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "y_true = mlb.fit_transform(ref_tokens)\n",
        "y_pred = mlb.transform(gen_tokens)\n",
        "\n",
        "precision = precision_score(y_true, y_pred, average='samples')\n",
        "recall = recall_score(y_true, y_pred, average='samples')\n",
        "f1 = f1_score(y_true, y_pred, average='samples')\n",
        "\n",
        "print(f\"\\nPrecision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1 Score:  {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovVPhlZ4jUR-",
        "outputId": "8966ac0f-bb4c-4aa1-c7d8-c7d2833fd9c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Precision: 0.7500\n",
            "Recall:    0.3542\n",
            "F1 Score:  0.4808\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:909: UserWarning: unknown class(es) ['at', 'banner.', 'looking', 'star', 'staring', 'young'] will be ignored\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}